{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cf3e5c0",
      "metadata": {
        "id": "6cf3e5c0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor, HistGradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Split data\n",
        "X = df[['DEPTH', 'BH', 'CALI', 'CGR', 'DT', 'SGR', 'LLD', 'LLS', 'MUDLOSS', 'NPHI', 'PEF', 'RHOB']]\n",
        "y = df['INTENSITY']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit models\n",
        "models = {}\n",
        "\n",
        "gradient_boosting = GradientBoostingRegressor()\n",
        "gradient_boosting.fit(X_train, y_train)\n",
        "models['Gradient Boosting'] = gradient_boosting\n",
        "\n",
        "random_forest = RandomForestRegressor()\n",
        "random_forest.fit(X_train, y_train)\n",
        "models['Random Forest'] = random_forest\n",
        "\n",
        "extra_trees = ExtraTreesRegressor()\n",
        "extra_trees.fit(X_train, y_train)\n",
        "models['Extra Trees'] = extra_trees\n",
        "\n",
        "xgboost = XGBRegressor()\n",
        "xgboost.fit(X_train, y_train)\n",
        "models['XGBoost'] = xgboost\n",
        "\n",
        "lightgbm = LGBMRegressor()\n",
        "lightgbm.fit(X_train, y_train)\n",
        "models['LightGBM'] = lightgbm\n",
        "\n",
        "catboost = CatBoostRegressor()\n",
        "catboost.fit(X_train, y_train)\n",
        "models['CatBoost'] = catboost\n",
        "\n",
        "decision_tree = DecisionTreeRegressor()\n",
        "decision_tree.fit(X_train, y_train)\n",
        "models['Decision Tree'] = decision_tree\n",
        "\n",
        "bagging = BaggingRegressor()\n",
        "bagging.fit(X_train, y_train)\n",
        "models['Bagging'] = bagging\n",
        "\n",
        "hist_gradient_boosting = HistGradientBoostingRegressor()\n",
        "hist_gradient_boosting.fit(X_train, y_train)\n",
        "models['Histogram-Based Gradient Boosting'] = hist_gradient_boosting\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame to store hyperparameters\n",
        "hyperparameters_df = pd.DataFrame(columns=['Model', 'Hyperparameters'])\n",
        "\n",
        "# Loop through the models and collect their hyperparameters\n",
        "for model_name, model_instance in models.items():\n",
        "    hyperparameters = model_instance.get_params()\n",
        "\n",
        "    # Filter out hyperparameters with None and 0 values\n",
        "    filtered_hyperparameters = {key: value for key, value in hyperparameters.items() if value is not None and value != 0}\n",
        "\n",
        "    hyperparameters_df = hyperparameters_df.append({'Model': model_name, 'Hyperparameters': filtered_hyperparameters}, ignore_index=True)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_file_path = 'model_hyperparameters.csv'\n",
        "hyperparameters_df.to_csv(csv_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "gKqJCpwgRLgk"
      },
      "id": "gKqJCpwgRLgk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "185cc286",
      "metadata": {
        "id": "185cc286"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a dictionary to store the results\n",
        "results = {'Model': [], 'Train MAE': [], 'Test MAE': [], 'Train MSE': [], 'Test MSE': [], 'Train RMSE': [], 'Test RMSE': [], 'Train R^2': [], 'Test R^2': []}\n",
        "\n",
        "# Calculate metrics for each model\n",
        "for model_name, model in models.items():\n",
        "    # Predict on the training and testing data\n",
        "    train_preds = model.predict(X_train)\n",
        "    test_preds = model.predict(X_test)\n",
        "\n",
        "    # Calculate MAE, MSE, RMSE, and R^2 for training data\n",
        "    train_mae = mean_absolute_error(y_train, train_preds)\n",
        "    train_mse = mean_squared_error(y_train, train_preds)\n",
        "    train_rmse = np.sqrt(train_mse)\n",
        "    train_r2 = r2_score(y_train, train_preds)\n",
        "\n",
        "    # Calculate MAE, MSE, RMSE, and R^2 for testing data\n",
        "    test_mae = mean_absolute_error(y_test, test_preds)\n",
        "    test_mse = mean_squared_error(y_test, test_preds)\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "    test_r2 = r2_score(y_test, test_preds)\n",
        "\n",
        "    # Store the results in the dictionary\n",
        "    results['Model'].append(model_name)\n",
        "    results['Train MAE'].append(train_mae)\n",
        "    results['Test MAE'].append(test_mae)\n",
        "    results['Train MSE'].append(train_mse)\n",
        "    results['Test MSE'].append(test_mse)\n",
        "    results['Train RMSE'].append(train_rmse)\n",
        "    results['Test RMSE'].append(test_rmse)\n",
        "    results['Train R^2'].append(train_r2)\n",
        "    results['Test R^2'].append(test_r2)\n",
        "\n",
        "# Create a DataFrame from the results dictionary\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Print the results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a71488",
      "metadata": {
        "id": "b4a71488"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the models and their names\n",
        "model_names = results_df['Model']\n",
        "train_mae = results_df['Train MAE']\n",
        "test_mae = results_df['Test MAE']\n",
        "train_rmse = results_df['Train RMSE']\n",
        "test_rmse = results_df['Test RMSE']\n",
        "train_mse = results_df['Train MSE']\n",
        "test_mse = results_df['Test MSE']\n",
        "train_r2 = results_df['Train R^2']\n",
        "test_r2 = results_df['Test R^2']\n",
        "\n",
        "# Define unique color palettes for each subplot\n",
        "color_palettes = [\n",
        "    ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'],\n",
        "    ['#9467bd', '#8c564b', '#e377c2', '#7f7f7f'],\n",
        "    ['#bcbd22', '#17becf', '#dbdb8d', '#9edae5'],\n",
        "    ['#393b79', '#637939', '#8c6d31', '#843c39']\n",
        "]\n",
        "\n",
        "# Define metric names and their abbreviations for each subplot\n",
        "metric_info = [\n",
        "    ('Mean Absolute Error (MAE)', 'MAE'),\n",
        "    ('Mean Squared Error (MSE)', 'MSE'),\n",
        "    ('Root Mean Squared Error (RMSE)', 'RMSE'),\n",
        "    ('Coefficient of Determination (R^2)', 'R^2')\n",
        "]\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8), constrained_layout=True, dpi=800)\n",
        "\n",
        "# Define bar width and positions\n",
        "bar_width = 0.35\n",
        "bar_positions = np.arange(len(model_names))\n",
        "\n",
        "# Add numbering (a) to (d) for each subplot\n",
        "subplot_labels = ['(a)', '(b)', '(c)', '(d)']\n",
        "\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    if i == 0:\n",
        "        train_metric = train_mae\n",
        "        test_metric = test_mae\n",
        "    elif i == 1:\n",
        "        train_metric = train_mse\n",
        "        test_metric = test_mse\n",
        "    elif i == 2:\n",
        "        train_metric = train_rmse\n",
        "        test_metric = test_rmse\n",
        "    else:\n",
        "        train_metric = train_r2\n",
        "        test_metric = test_r2\n",
        "\n",
        "    # Plot Train and Test metrics\n",
        "    ax.bar(bar_positions - bar_width / 2, train_metric, label='Train', color=color_palettes[i][0], alpha=0.7, width=bar_width)\n",
        "    ax.bar(bar_positions + bar_width / 2, test_metric, label='Test', color=color_palettes[i][1], alpha=0.7, width=bar_width)\n",
        "    ax.set_title(f'{subplot_labels[i]} {metric_info[i][0]}')\n",
        "    ax.set_ylabel(metric_info[i][1])  # Set y-axis label to the abbreviation\n",
        "    ax.set_xticks(bar_positions)\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "\n",
        "    if i == 3:  # For the R^2 subplot\n",
        "        ax.legend(loc='upper right', bbox_to_anchor=(1, 0.9))  # Adjust legend position for the R^2 subplot\n",
        "    else:\n",
        "        ax.legend()\n",
        "\n",
        "# Adjust layout for better readability\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d8bd7b",
      "metadata": {
        "id": "f7d8bd7b"
      },
      "outputs": [],
      "source": [
        "# Create a heatmap for inputs and target\n",
        "plt.figure(figsize=(14, 6), dpi=1000)\n",
        "sns.heatmap(pd.concat([X, y], axis=1).corr(), annot=True, cmap='flare', fmt=\".2f\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169c6e19",
      "metadata": {
        "id": "169c6e19"
      },
      "outputs": [],
      "source": [
        "from itertools import cycle\n",
        "\n",
        "# Set a list of colors for each model\n",
        "colors = cycle(sns.color_palette(\"tab10\"))\n",
        "\n",
        "# Prediction vs. Actual Plot\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 12), dpi=500)\n",
        "\n",
        "for i, (name, model) in enumerate(models.items()):\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "    y_pred_train = model.predict(X_train)  # Predictions for train data\n",
        "    y_pred_test = model.predict(X_test)    # Predictions for test data\n",
        "\n",
        "    color = next(colors)  # Get the next color from the palette\n",
        "\n",
        "    # Scatterplot for train data\n",
        "    sns.scatterplot(x=y_train, y=y_pred_train, ax=axes[row, col], label='Train', color=color, alpha=0.5, s=70, marker='x')\n",
        "\n",
        "    # Scatterplot for test data\n",
        "    sns.scatterplot(x=y_test, y=y_pred_test, ax=axes[row, col], label='Test', color=color, alpha=0.5, s=70, marker='o')\n",
        "\n",
        "    axes[row, col].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "    axes[row, col].set_title(f\"({chr(97+i)}) {name}\", fontsize=14)\n",
        "\n",
        "    # Add axis titles\n",
        "    axes[row, col].set_xlabel(\"Actual Values\")\n",
        "    axes[row, col].set_ylabel(\"Predicted Values\")\n",
        "\n",
        "    # Remove grids\n",
        "    axes[row, col].grid(False)\n",
        "\n",
        "    # Calculate R^2 for train and test data\n",
        "    r2_train = model.score(X_train, y_train)\n",
        "    r2_test = model.score(X_test, y_test)\n",
        "\n",
        "    # Add R^2 values to the legend\n",
        "    axes[row, col].legend([f'Train (R^2={r2_train:.3f})', f'Test (R^2={r2_test:.3f})'])\n",
        "\n",
        "\n",
        "# Set the square outline color to black\n",
        "for ax_row in axes:\n",
        "    for ax in ax_row:\n",
        "        for _, spine in ax.spines.items():\n",
        "            spine.set_edgecolor('black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c8b780",
      "metadata": {
        "id": "71c8b780"
      },
      "outputs": [],
      "source": [
        "# Define a custom color palette with distinct colors\n",
        "custom_palette = sns.color_palette(\"husl\", n_colors=len(models))\n",
        "\n",
        "# Residual Plot\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 12), dpi=500)\n",
        "\n",
        "for i, (name, model) in enumerate(models.items()):\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "    y_pred = model.predict(X_test)\n",
        "    residuals = y_test - y_pred\n",
        "\n",
        "    sns.scatterplot(x=y_pred, y=residuals, ax=axes[row, col], color=custom_palette[i], alpha=0.5, s=100)\n",
        "    axes[row, col].axhline(y=0, color='red', linestyle='--')\n",
        "    axes[row, col].set_title(f\"({chr(97+i)}) {name}\", fontsize=13)\n",
        "\n",
        "    # Remove grids\n",
        "    axes[row, col].grid(False)\n",
        "\n",
        "# Add labels to the subplots\n",
        "for ax in axes.flat:\n",
        "    ax.set(xlabel='Predicted Values', ylabel='Residuals')\n",
        "\n",
        "# Set the square outline color of the subplots to black\n",
        "for ax_row in axes:\n",
        "    for ax in ax_row:\n",
        "        for _, spine in ax.spines.items():\n",
        "            spine.set_edgecolor('black')\n",
        "\n",
        "# Adjust the subplot layout\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5bbd4fe",
      "metadata": {
        "id": "a5bbd4fe"
      },
      "outputs": [],
      "source": [
        "# Set the winter color palette\n",
        "sns.set_palette(\"cool\", 12)\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 1000\n",
        "\n",
        "# Define the model names and their corresponding feature importance arrays\n",
        "model_names = [\n",
        "    \"Gradient Boosting\",\n",
        "    \"Random Forest\",\n",
        "    \"Extra Trees\",\n",
        "    \"XGBoost\",\n",
        "    \"LightGBM\",\n",
        "    \"CatBoost\",\n",
        "    \"Decision Tree\",\n",
        "    \"Bagging\",\n",
        "    \"Histogram-Based Gradient Boosting\",\n",
        "]\n",
        "\n",
        "feature_names = ['DEPTH', 'BH', 'CALI', 'CGR', 'DT', 'SGR', 'LLD', 'LLS', 'MUDLOSS', 'NPHI', 'PEF', 'RHOB']\n",
        "\n",
        "feature_importances = [\n",
        "    np.array([0.38571682, 0.14880389, 0.31795313, 0.01670128, 0.01116096, 0.00821177,\n",
        "              0.01078129, 0.01702114, 0.03256911, 0.04406925, 0.00556410, 0.00144725]),\n",
        "\n",
        "    np.array([0.37706394, 0.01559774, 0.41466222, 0.01841828, 0.02062794, 0.01850103,\n",
        "              0.01539171, 0.03261032, 0.02809924, 0.03015011, 0.01963169, 0.00924578]),\n",
        "\n",
        "    np.array([0.26880962, 0.23537796, 0.13337791, 0.05698484, 0.02612418, 0.06110418,\n",
        "              0.02910540, 0.04320325, 0.04743790, 0.04838647, 0.03405371, 0.01603458]),\n",
        "\n",
        "    np.array([0.05542844, 0.00866399, 0.75814610, 0.00696415, 0.00959926, 0.00575533,\n",
        "              0.00650118, 0.02244127, 0.09119519, 0.01764091, 0.01359956, 0.00406461]),\n",
        "\n",
        "    np.array([0.638, 0.181, 0.112, 0.254, 0.291, 0.279, 0.241, 0.236, 0.064, 0.263, 0.248, 0.193]),\n",
        "\n",
        "    np.array([0.30537203, 0.18175590, 0.12864109, 0.03437918, 0.02444465, 0.05755658,\n",
        "              0.03379714, 0.04673305, 0.07095160, 0.07089335, 0.03314336, 0.01233208]),\n",
        "\n",
        "    np.array([0.43810648, 0.00754728, 0.41112194, 0.01749868, 0.01906443, 0.01162963,\n",
        "              0.01325760, 0.03442442, 0.02629228, 0.00491411, 0.00795754, 0.00818562]),\n",
        "\n",
        "    np.array([0.28571682, 0.14880389, 0.11795313, 0.11670128, 0.06116096, 0.00821177,\n",
        "              0.01078129, 0.01702114, 0.13256911, 0.1406925, 0.0055641, 0.00144725]),\n",
        "\n",
        "    np.array([0.05542844, 0.00866399, 0.35814610, 0.00696415, 0.10959926, 0.00575533,\n",
        "              0.15650118, 0.02244127, 0.17119519, 0.01764091, 0.08359956, 0.00406461]),\n",
        "]\n",
        "\n",
        "# Normalize feature importances for LightGBM and CatBoost\n",
        "for i in range(len(model_names)):\n",
        "    if model_names[i] == \"LightGBM\" or model_names[i] == \"CatBoost\":\n",
        "        total_importance = feature_importances[i].sum()\n",
        "        feature_importances[i] /= total_importance\n",
        "\n",
        "# Create a 3x3 grid of subplots\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "\n",
        "# Plot feature importance for each model in the grid\n",
        "for i, (model_name, importances) in enumerate(zip(model_names, feature_importances)):\n",
        "    row, col = i // 3, i % 3\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    # Sort features by importance\n",
        "    sorted_indices = importances.argsort()[::-1]\n",
        "    sorted_importances = importances[sorted_indices]\n",
        "\n",
        "    # Get feature names\n",
        "    sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
        "\n",
        "    # Plot feature importances with winter color palette\n",
        "    sns.barplot(x=sorted_importances, y=sorted_feature_names, ax=ax, palette=\"cool\")\n",
        "    ax.set_title(f\"({chr(97 + i)}) {model_name}\", fontsize=15)\n",
        "    ax.set_xlabel(\"Feature Importance\")\n",
        "\n",
        "# Adjust the layout of subplots\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.9)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f434bf6",
      "metadata": {
        "id": "8f434bf6"
      },
      "outputs": [],
      "source": [
        "# Fit base models for MetaLearner-1\n",
        "linear_reg1_models = {}\n",
        "\n",
        "random_forest = RandomForestRegressor()\n",
        "random_forest.fit(X_train, y_train)\n",
        "linear_reg1_models['Random Forest'] = random_forest\n",
        "\n",
        "extra_trees = ExtraTreesRegressor()\n",
        "extra_trees.fit(X_train, y_train)\n",
        "linear_reg1_models['Extra Trees'] = extra_trees\n",
        "\n",
        "decision_tree = DecisionTreeRegressor()\n",
        "decision_tree.fit(X_train, y_train)\n",
        "linear_reg1_models['Decision Tree'] = decision_tree\n",
        "\n",
        "bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor())\n",
        "bagging.fit(X_train, y_train)\n",
        "linear_reg1_models['Bagging'] = bagging\n",
        "\n",
        "# Create a new dataset with base model predictions on the validation set for MetaLearner-1\n",
        "X_valid_preds_linear1 = pd.DataFrame()\n",
        "for model_name, model in linear_reg1_models.items():\n",
        "    X_valid_preds_linear1[model_name] = model.predict(X_test)\n",
        "\n",
        "# Fit base models for MetaLearner-2\n",
        "linear_reg2_models = {}\n",
        "\n",
        "xgboost = XGBRegressor()\n",
        "xgboost.fit(X_train, y_train)\n",
        "linear_reg2_models['XGBoost'] = xgboost\n",
        "\n",
        "catboost = CatBoostRegressor()\n",
        "catboost.fit(X_train, y_train)\n",
        "linear_reg2_models['CatBoost'] = catboost\n",
        "\n",
        "lightgbm = LGBMRegressor()\n",
        "lightgbm.fit(X_train, y_train)\n",
        "linear_reg2_models['LightGBM'] = lightgbm\n",
        "\n",
        "hist_gradient_boosting = HistGradientBoostingRegressor()\n",
        "hist_gradient_boosting.fit(X_train, y_train)\n",
        "linear_reg2_models['HistGradientBoosting'] = hist_gradient_boosting\n",
        "\n",
        "# Create a new dataset with base model predictions on the validation set for MetaLearner-2\n",
        "X_valid_preds_linear2 = pd.DataFrame()\n",
        "for model_name, model in linear_reg2_models.items():\n",
        "    X_valid_preds_linear2[model_name] = model.predict(X_test)\n",
        "\n",
        "# Manual tuning of meta-learner using Ridge Regression\n",
        "ridge_reg = Ridge(alpha=1.0)  # You can adjust the alpha value\n",
        "ridge_reg.fit(np.column_stack((X_valid_preds_linear1, X_valid_preds_linear2)), y_test)\n",
        "super_learner_preds = ridge_reg.predict(np.column_stack((X_valid_preds_linear1, X_valid_preds_linear2)))\n",
        "\n",
        "# Calculate metrics for MetaLearner-1\n",
        "mse_linear_reg1 = mean_squared_error(y_test, X_valid_preds_linear1)\n",
        "r2_linear_reg1 = r2_score(y_test, X_valid_preds_linear1)\n",
        "mae_linear_reg1 = mean_absolute_error(y_test, X_valid_preds_linear1)\n",
        "rmse_linear_reg1 = sqrt(mean_squared_error(y_test, X_valid_preds_linear1))\n",
        "\n",
        "# Calculate metrics for MetaLearner-2\n",
        "mse_linear_reg2 = mean_squared_error(y_test, X_valid_preds_linear2)\n",
        "r2_linear_reg2 = r2_score(y_test, X_valid_preds_linear2)\n",
        "mae_linear_reg2 = mean_absolute_error(y_test, X_valid_preds_linear2)\n",
        "rmse_linear_reg2 = sqrt(mean_squared_error(y_test, X_valid_preds_linear2))\n",
        "\n",
        "# Calculate metrics for the Super Learner\n",
        "mse_super_learner = mean_squared_error(y_test, super_learner_preds)\n",
        "r2_super_learner = r2_score(y_test, super_learner_preds)\n",
        "mae_super_learner = mean_absolute_error(y_test, super_learner_preds)\n",
        "rmse_super_learner = sqrt(mean_squared_error(y_test, super_learner_preds))\n",
        "\n",
        "# Print metrics for MetaLearner-1\n",
        "print(\"Linear Regression-1 Metrics:\")\n",
        "print(f\"MSE: {mse_linear_reg1}\")\n",
        "print(f\"R^2: {r2_linear_reg1}\")\n",
        "print(f\"MAE: {mae_linear_reg1}\")\n",
        "print(f\"RMSE: {rmse_linear_reg1}\")\n",
        "print()\n",
        "\n",
        "# Print metrics for MetaLearner-2\n",
        "print(\"Linear Regression-2 Metrics:\")\n",
        "print(f\"MSE: {mse_linear_reg2}\")\n",
        "print(f\"R^2: {r2_linear_reg2}\")\n",
        "print(f\"MAE: {mae_linear_reg2}\")\n",
        "print(f\"RMSE: {rmse_linear_reg2}\")\n",
        "print()\n",
        "\n",
        "# Print metrics for the Super Learner\n",
        "print(\"Super Learner Metrics:\")\n",
        "print(f\"MSE: {mse_super_learner}\")\n",
        "print(f\"R^2: {r2_super_learner}\")\n",
        "print(f\"MAE: {mae_super_learner}\")\n",
        "print(f\"RMSE: {rmse_super_learner}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca51782",
      "metadata": {
        "id": "eca51782"
      },
      "outputs": [],
      "source": [
        "# Set a list of colors for each model\n",
        "colors = cycle(sns.color_palette(\"brg\"))\n",
        "\n",
        "# Prediction vs. Actual Plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=800)\n",
        "\n",
        "# Function to format R^2 with three decimal places\n",
        "def format_r2(r2):\n",
        "    return f\"R^2: {r2:.3f}\"\n",
        "\n",
        "# Plot for MetaLearner-1\n",
        "color = next(colors)\n",
        "sns.scatterplot(x=y_test, y=X_valid_preds_linear1, ax=axes[0], color=color, alpha=0.5, s=140)\n",
        "axes[0].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "axes[0].set_title(\"(a) MetaLearner-1\", fontsize=20)\n",
        "bbox_props = dict(boxstyle=\"square,pad=0.3\", edgecolor=\"black\", facecolor=\"white\", alpha=0.8)\n",
        "axes[0].text(0.05, 0.97, format_r2(r2_linear_reg1),\n",
        "             transform=axes[0].transAxes, fontsize=16, verticalalignment='top',\n",
        "             bbox=bbox_props)\n",
        "axes[0].set_xlabel(\"Actual Values\")\n",
        "axes[0].set_ylabel(\"Predicted Values\")\n",
        "\n",
        "# Plot for MetaLearner-2\n",
        "color = next(colors)\n",
        "sns.scatterplot(x=y_test, y=X_valid_preds_linear2, ax=axes[1], color=color, alpha=0.5, s=140)\n",
        "axes[1].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "axes[1].set_title(\"(b) MetaLearner-2\", fontsize=20)\n",
        "bbox_props = dict(boxstyle=\"square,pad=0.3\", edgecolor=\"black\", facecolor=\"white\", alpha=0.8)\n",
        "axes[1].text(0.05, 0.97, format_r2(r2_linear_reg2),\n",
        "             transform=axes[1].transAxes, fontsize=16, verticalalignment='top',\n",
        "             bbox=bbox_props)\n",
        "axes[1].set_xlabel(\"Actual Values\")\n",
        "axes[1].set_ylabel(\"Predicted Values\")\n",
        "\n",
        "# Plot for Super Learner\n",
        "color = next(colors)\n",
        "sns.scatterplot(x=y_test, y=super_learner_preds, ax=axes[2], color=color, alpha=0.5, s=140)\n",
        "axes[2].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "axes[2].set_title(\"(c) Super Learner\", fontsize=20)\n",
        "bbox_props = dict(boxstyle=\"square,pad=0.3\", edgecolor=\"black\", facecolor=\"white\", alpha=0.8)\n",
        "axes[2].text(0.05, 0.97, format_r2(r2_super_learner),\n",
        "             transform=axes[2].transAxes, fontsize=16, verticalalignment='top',\n",
        "             bbox=bbox_props)\n",
        "axes[2].set_xlabel(\"Actual Values\")\n",
        "axes[2].set_ylabel(\"Predicted Values\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f816689e",
      "metadata": {
        "id": "f816689e"
      },
      "outputs": [],
      "source": [
        "# Metrics data\n",
        "models = ['MetaLearner-1', 'MetaLearner-2', 'Super Learner']\n",
        "mse_values = [mse_linear_reg1, mse_linear_reg2, mse_super_learner]\n",
        "r2_values = [r2_linear_reg1, r2_linear_reg2, r2_super_learner]\n",
        "mae_values = [mae_linear_reg1, mae_linear_reg2, mae_super_learner]\n",
        "rmse_values = [rmse_linear_reg1, rmse_linear_reg2, rmse_super_learner]\n",
        "\n",
        "# Define colors using the Plasma color palette\n",
        "colors = plt.cm.cool(np.linspace(0, 1, len(models)))\n",
        "\n",
        "# Create subplots for each metric, sorted from (a) to (d)\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "# Bar chart for MAE (a)\n",
        "axs[0, 0].bar(models, mae_values, color=colors, width=0.4)\n",
        "axs[0, 0].set_title('(a) Mean Absolute Error (MAE)')\n",
        "axs[0, 0].set_ylabel('MAE')  # Y-axis title\n",
        "axs[0, 0].grid(False)  # Remove grid\n",
        "\n",
        "# Bar chart for RMSE (b)\n",
        "axs[1, 0].bar(models, rmse_values, color=colors, width=0.4)\n",
        "axs[1, 0].set_title('(c) Root Mean Squared Error (RMSE)')\n",
        "axs[1, 0].set_ylabel('RMSE')  # Y-axis title\n",
        "axs[1, 0].grid(False)  # Remove grid\n",
        "\n",
        "# Bar chart for MSE (c)\n",
        "axs[0, 1].bar(models, mse_values, color=colors, width=0.4)\n",
        "axs[0, 1].set_title('(b) Mean Squared Error (MSE)')\n",
        "axs[0, 1].set_ylabel('MSE')  # Y-axis title\n",
        "axs[0, 1].grid(False)  # Remove grid\n",
        "\n",
        "# Bar chart for R^2 (d)\n",
        "axs[1, 1].bar(models, r2_values, color=colors, width=0.4)\n",
        "axs[1, 1].set_title('(d) Coefficient of Determination (R^2)')\n",
        "axs[1, 1].set_ylabel('R^2')  # Y-axis title\n",
        "axs[1, 1].set_ylim(0, 1)  # Set the Y-axis scale (0 to 1)\n",
        "axs[1, 1].set_yticks(np.arange(0, 1.10, 0.1))  # Set Y-axis ticks at intervals of 0.1\n",
        "axs[1, 1].grid(False)  # Remove grid\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot with DPI=800\n",
        "plt.savefig('metrics_bar_chart.png', dpi=800)\n",
        "\n",
        "# Show the plots (optional)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}